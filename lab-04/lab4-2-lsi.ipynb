{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2: Latent semantic indexing\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *H*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Baffou Jérémy*\n",
    "* *Basseto Antoine*\n",
    "* *Pinto Andrea*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 2 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the Term-Document matrix computed in the first part of the lab. We also keep track of the mappings we used, and their values for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (206210,854)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-acd921067c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TFIDF.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"terms_ids.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mterms_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"course_ids.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (206210,854)"
     ]
    }
   ],
   "source": [
    "TFIDF = np.load(\"sparse_TFIDF.npy\")\n",
    "\n",
    "a_file = open(\"terms_ids.pkl\", \"rb\")\n",
    "terms_ids = pickle.load(a_file)\n",
    "b_file = open(\"course_ids.pkl\", \"rb\")\n",
    "course_ids = pickle.load(b_file)\n",
    "\n",
    "terms_keys = list(terms_ids.keys())\n",
    "doc_keys = list(course_ids.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Latent semantic indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function svds from scipy.sparse.linalg to compute the SVD with k=300 has the targeted rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v_T = svds(TFIDF,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this function is that the output matrices are not well ordered. We need to manipulate them a bit to retrieve the correct matrices. After experimentation and comparison with the function np.linalg.svd(), we have obtain the following transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = -u[:,::-1]*np.asarray([-1]+np.ones(u.shape[1]-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = s[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_T = (v_T[::-1].T*np.asarray([1]+(-1 * np.ones(v_T.shape[0]-1)).tolist())).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $U$ give \"eigenvectors\" of the correlation matrix row-wise of the TF-IDF matrix (so based on words), ordered in decreasing order of variance they capture. The columns of $V^T$, i.e. rows of $V$, are the \"eigenvectors\" of the correlation matrix column-wise of the TF_IDF matrix (so based on documents), ordered in decreasing order of variance they capture. The value of S are the singular values of the TF-IDF matrix, which indicate how much an association of vectors of U and V are necessary to give back original vectors of TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for singular_value in S[:20]:\n",
    "    print(singular_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that by the SVD we mapped our documents into a smaller rank matrix, and thus a sub-space called the latent space. We know that the columns of $U$,$S$,$V_T$ are ordered in there order of importance (i.e. amount of variance of the original data they capture). Thus the first **column** of $U$ will give a \"doc\" where the combination of terms capture a lot of variance. Same thing for the first **row** of $V$, it gives a weighted group of docs which captures a lot of variance. Thus to select our topic we keep the max terms (i.e. weights) in the first 10 columns of $U$ and first 10 rows of $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extraction(index):\n",
    "    best_terms_indices = U[:,index].argsort()[-10:][::-1]\n",
    "    print(\"Terms in topic :\")\n",
    "    #for j in best_terms_indices:\n",
    "    print(list(map(lambda l : terms_keys[l],best_terms_indices)))\n",
    "    print(\"Courses in topic :\")\n",
    "    best_courses_indices = V_T[index,:].argsort()[-10:][::-1]\n",
    "    #for k in best_courses_indices:\n",
    "    print(list(map(lambda l : doc_keys[l], best_courses_indices)))\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"The {i+1}th topic is composed of :\")\n",
    "    topic_extraction(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give the following titles for the 10 topics:\n",
    "- Chemistry\n",
    "- Computer Science Project\n",
    "- Administration\n",
    "- Ph.D project\n",
    "- Semester project\n",
    "- Algebra (Fields)\n",
    "- Algebra\n",
    "- Plasma physics\n",
    "- Manufacturing of micro-components\n",
    "- Manufacturing of micro-components\n",
    "\n",
    "Note that some topics are very similar, which is why we gave them the same titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Document similarity search in concept-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a similarity function between a term and a document like stated in the handout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(t,d):\n",
    "    output = U[t,:] @ np.diag(S)  @ V_T[:,d]\n",
    "    norm_factor = np.linalg.norm(U[t,:]) * np.linalg.norm(np.diag(S) @ V_T[:,d])\n",
    "    return output/norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a search function wich computes the top num courses that matches the list of words given. Note that the aggregation of similarity by a plus operation is not really reliable for documents comparisons, but we wanted to have a look for only a few terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_function(words_list,num=5):\n",
    "    query_result = np.zeros(V_T.shape[1])\n",
    "    for i in range(V_T.shape[1]):\n",
    "        for j in range(len(words_list)):\n",
    "            query_result[i] += sim(terms_ids[words_list[j]],i)\n",
    "    best_fit = query_result.argsort()[-num:][::-1]\n",
    "    for k in best_fit:\n",
    "        print(f\"Course ID : {doc_keys[k]}, Similarity Score : {query_result[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_function([\"facebook\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"facebook\" the results are much better! Courses returned now have to do with social media in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_function([\"markov chain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"markov chain\" the results are really quite similar to the previous part, and still coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we do a query similar to what we have done in the previous part :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_function([\"facebook\",\"markov chain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are really great, better than with only the TF-IDF approach. It captures well the idea of social network and so gives enough importance to facebook so that relevent courses are more on top than before (e.g. EE-593)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Document-document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we use to compare two courses in our latent space is : cos-sim$(S\\cdot V^T_{d1},S\\cdot V^T_{d2})$, where $V^T_{di}$ indicates the i-th column vector from $V^T$, and cos-sim is the cosine similarity :\n",
    "\n",
    "cos-sim(v1,v2) = $\\frac{v1 \\cdot v2}{||v1||*||v2||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cosine similarity between two documents. Take their two ids as parameters.\n",
    "\"\"\"\n",
    "def sim_documents(d1,d2):\n",
    "    doc_1 = np.diag(S) @ V_T[:,d1]\n",
    "    doc_2 = np.diag(S) @ V_T[:,d2]\n",
    "    return (doc_1 @ doc_2)/(np.linalg.norm(doc_1)*np.linalg.norm(doc_2))\n",
    "\n",
    "\"\"\"\n",
    "Take a course and output the num courses the closest to it in the latent space (in the cosine sense)\n",
    "\"\"\"\n",
    "def course_recommender(course,num=5):\n",
    "    course_id = course_ids[course]\n",
    "    corpus_similarity = np.zeros(TFIDF.shape[1])\n",
    "    for i in range(TFIDF.shape[1]):\n",
    "        if i != course_id:\n",
    "            corpus_similarity[i] = sim_documents(i,course_id)\n",
    "    best_fit = corpus_similarity.argsort()[-num:][::-1]\n",
    "    for k in best_fit:\n",
    "        print(doc_keys[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_recommender(\"COM-308\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendation are actually pretty great! (Except maybe the last one which does not exactly capture the essence of the course, but still it is an important course)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
