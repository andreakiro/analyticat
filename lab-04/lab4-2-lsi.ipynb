{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 2: Latent semantic indexing\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *H*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Baffou Jérémy*\n",
    "* *Basseto Antoine*\n",
    "* *Pinto Andrea*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 2 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the Term-Document matrix computed in the first part of the lab. We also keep track of the mappings we used, and their values for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = np.load(\"TFIDF.npy\")\n",
    "\n",
    "a_file = open(\"terms_ids.pkl\", \"rb\")\n",
    "terms_ids = pickle.load(a_file)\n",
    "b_file = open(\"course_ids.pkl\", \"rb\")\n",
    "course_ids = pickle.load(b_file)\n",
    "\n",
    "terms_keys = list(terms_ids.keys())\n",
    "doc_keys = list(course_ids.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Latent semantic indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the function svds from scipy.sparse.linalg to compute the SVD with k=300 has the targeted rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s,v_T = svds(TFIDF,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this function is that the output matrices are not well ordered. We need to manipulate them a bit to retrieve the correct matrices. After experimentation and comparison with the function np.linalg.svd(), we have obtain the following transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = -u[:,::-1]*np.asarray([-1]+np.ones(u.shape[1]-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = s[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_T = (v_T[::-1].T*np.asarray([1]+(-1 * np.ones(v_T.shape[0]-1)).tolist())).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of $U$ give \"eigenvectors\" of the correlation matrix row-wise of the TF-IDF matrix (so based on words), ordered in decreasing order of variance they capture. The columns of $V^T$, i.e. rows of $V$, are the \"eigenvectors\" of the correlation matrix column-wise of the TF_IDF matrix (so based on documents), ordered in decreasing order of variance they capture. The value of S are the singular values of the TF-IDF matrix, which indicate how much an association of vectors of U and V are necessary to give back original vectors of TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1960467509505595\n",
      "3.1372127474371307\n",
      "2.9790206315696195\n",
      "2.1023634847769848\n",
      "1.191793531134566\n",
      "1.168869187010023\n",
      "1.1194782959086742\n",
      "1.0715814332815714\n",
      "1.048234637445236\n",
      "1.0411024305240602\n",
      "1.0310886533235462\n",
      "1.0257202178029305\n",
      "0.9993423732670951\n",
      "0.9448882250539762\n",
      "0.9312196332599237\n",
      "0.9136444774029263\n",
      "0.906949117562127\n",
      "0.8973318146743732\n",
      "0.8955685993325307\n",
      "0.8899137209771941\n"
     ]
    }
   ],
   "source": [
    "for singular_value in S[:20]:\n",
    "    print(singular_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @pinto et basseto \n",
    "\n",
    "Je sais pas si ici il faut donner pour chaque topic 10 terms et 10 docs ou si il faut donner **en tout** 10 terms et 10 docs (donc 1 de chaque par topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that by the SVD we mapped our documents into a smaller rank matrix, and thus a sub-space called the latent space. We know that the columns of $U$,$S$,$V_T$ are ordered in there order of importance (i.e. amount of variance of the original data they capture). Thus the first **column** of $U$ will give a \"doc\" where the combination of terms capture a lot of variance. Same thing for the first **row** of $V$, it gives a weighted group of docs which captures a lot of variance. Thus to select our topic we keep the max terms (i.e. weights) in the first 10 columns of $U$ and first 10 rows of $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extraction(index):\n",
    "    best_terms_indices = U[:,index].argsort()[-10:][::-1]\n",
    "    print(\"Terms in topic :\")\n",
    "    #for j in best_terms_indices:\n",
    "    print(list(map(lambda l : terms_keys[l],best_terms_indices)))\n",
    "    print(\"Courses in topic :\")\n",
    "    best_courses_indices = V_T[index,:].argsort()[-10:][::-1]\n",
    "    #for k in best_courses_indices:\n",
    "    print(list(map(lambda l : doc_keys[l], best_courses_indices)))\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1th topic is composed of :\n",
      "Terms in topic :\n",
      "['dilution solution', 'predict major', 'content introduction dilute', 'rubber transition rubber', 'concetrated solution glass', 'concetrated solution bulk', 'rubber transition', 'principle result', 'principle result chainlike', 'concentrate solution phase']\n",
      "Courses in topic :\n",
      "['CH-332', 'MGT-690(B)', 'MGT-690(A)', 'MSE-431', 'PHYS-708', 'PHYS-709', 'PHYS-610', 'CH-710', 'COM-404', 'ME-705']\n",
      "--------------------------------\n",
      "The 2th topic is composed of :\n",
      "Terms in topic :\n",
      "['project ic laboratory', 'project ic', 'ic laboratory', 'semester project ic', 'semester project', 'ic', 'semiconductor device wiley', 'scale ballistic', 'outline schematic', 'outline schematic layout']\n",
      "Courses in topic :\n",
      "['CS-699(2)', 'CS-699(1)', 'MICRO-432', 'PHYS-709', 'MGT-609', 'MATH-400', 'MGT-690(B)', 'MGT-690(A)', 'PHYS-731', 'PHYS-708']\n",
      "--------------------------------\n",
      "The 3th topic is composed of :\n",
      "Terms in topic :\n",
      "['administration enrollment', 'edmt administration enrollment', 'contact edmt', 'contact edmt administration', 'edmt', 'edmt administration', 'administration', 'enrollment', 'biometric', 'pharmacogenetics']\n",
      "Courses in topic :\n",
      "['MGT-690(B)', 'MGT-690(A)', 'BIO-478', 'EE-729', 'CH-603', 'CH-602', 'EE-612', 'BIO-494', 'CH-628', 'CH-411']\n",
      "--------------------------------\n",
      "The 4th topic is composed of :\n",
      "Terms in topic :\n",
      "['rotation', 'training rotation', 'train rotation', 'program director', 'procedure consist project', 'train rotation write', 'train rotation place', 'month total examination', 'mean intend achieve', 'mean intend']\n",
      "Courses in topic :\n",
      "['BIO-699(n)', 'BIO-699(m)', 'CS-699(2)', 'CS-699(1)', 'MGT-690(B)', 'MGT-690(A)', 'MATH-625', 'MGT-707', 'MGT-635', 'PHYS-709']\n",
      "--------------------------------\n",
      "The 5th topic is composed of :\n",
      "Terms in topic :\n",
      "['semester project ic', 'project ic', 'ic laboratory', 'project ic laboratory', 'training rotation', 'rotation', 'administration enrollment', 'contact edmt administration', 'contact edmt', 'edmt']\n",
      "Courses in topic :\n",
      "['CS-699(1)', 'CS-699(2)', 'BIO-699(n)', 'MGT-690(B)', 'MGT-690(A)', 'CH-332', 'MICRO-606', 'COM-712', 'CS-470', 'CIVIL-704']\n",
      "--------------------------------\n",
      "The 6th topic is composed of :\n",
      "Terms in topic :\n",
      "['additive', 'integer finite', 'question additive', 'field revolve', 'combinatorics field revolve', 'combinatorics field', 'combinatorial question additive', 'combinatorial question', 'study result additive', 'study result']\n",
      "Courses in topic :\n",
      "['MATH-636', 'MSE-804', 'MSE-657', 'MATH-260', 'MATH-409', 'FIN-603', 'COM-102', 'ME-415', 'CH-611', 'MATH-460']\n",
      "--------------------------------\n",
      "The 7th topic is composed of :\n",
      "Terms in topic :\n",
      "['additive', 'integer finite field', 'additive group', 'combinatorial question', 'combinatorial question additive', 'combinatorics field', 'combinatorics field revolve', 'emphasis integer finite', 'emphasis integer', 'study result']\n",
      "Courses in topic :\n",
      "['MATH-636', 'MGT-609', 'BIO-699(n)', 'CS-699(2)', 'CS-699(1)', 'MGT-690(A)', 'MGT-690(B)', 'CH-332', 'MSE-804', 'MGT-552']\n",
      "--------------------------------\n",
      "The 8th topic is composed of :\n",
      "Terms in topic :\n",
      "['diagnostics', 'process method modern', 'fusion experiment', 'theory experimental practice', 'method modern fusion', 'learn plasma', 'learn plasma diagnostics', 'bridge gap diagnostics', 'diagnostics data', 'diagnostics data process']\n",
      "Courses in topic :\n",
      "['PHYS-732', 'PHYS-731', 'BIO-469', 'PHYS-424', 'PHYS-445', 'BIOENG-801', 'EE-706', 'PHYS-448', 'MATH-641', 'MSE-470(b)']\n",
      "--------------------------------\n",
      "The 9th topic is composed of :\n",
      "Terms in topic :\n",
      "['arrive future', 'govern assembly', 'expect arrive future', 'assembly describe', 'assembly describe concrete', 'assembly place', 'aspect govern assembly', 'assembly place expect', 'micro assembly place', 'example field manufacture']\n",
      "Courses in topic :\n",
      "['MICRO-620', 'MICRO-707', 'AR-202(c)', 'MATH-428', 'PHYS-731', 'EE-516', 'AR-201(c)', 'PHYS-709', 'MSE-643', 'MGT-404']\n",
      "--------------------------------\n",
      "The 10th topic is composed of :\n",
      "Terms in topic :\n",
      "['manufacture micro', 'arrive future', 'micro assembly place', 'micro assembly', 'describe concrete', 'example field manufacture', 'manufacture micro assembly', 'describe concrete application', 'assembly place expect', 'assembly place']\n",
      "Courses in topic :\n",
      "['MICRO-620', 'PHYS-732', 'MATH-636', 'MATH-625', 'MGT-609', 'CS-699(1)', 'CS-699(2)', 'MICRO-707', 'BIO-699(n)', 'MGT-690(A)']\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"The {i+1}th topic is composed of :\")\n",
    "    topic_extraction(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give the following titles for the 10 topics:\n",
    "- Chemistry\n",
    "- Computer Science Project\n",
    "- Administration ?\n",
    "- Ph.D project\n",
    "- Ph.D project\n",
    "- Field Theory\n",
    "- Combinatorial Field\n",
    "- Physic of Plasma\n",
    "- Manufacturing of micro-components\n",
    "- Manufacturing of micro-components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Document similarity search in concept-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a similarity function between a term and a document like stated in the handout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(t,d):\n",
    "    output = U[t,:] @ np.diag(S)  @ V_T[:,d]\n",
    "    norm_factor = np.linalg.norm(U[t,:]) * np.linalg.norm(np.diag(S) @ V_T[:,d])\n",
    "    return output/norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a search function wich computes the top num courses that matches the list of words given. Note that the aggregation of similarity by a plus operation is not really reliable for documents comparisons, but we wanted to have a look for only a few terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_function(words_list,num=5):\n",
    "    query_result = np.zeros(V_T.shape[1])\n",
    "    for i in range(V_T.shape[1]):\n",
    "        for j in range(len(words_list)):\n",
    "            query_result[i] += sim(terms_ids[words_list[j]],i)\n",
    "    best_fit = query_result.argsort()[-num:][::-1]\n",
    "    for k in best_fit:\n",
    "        print(f\"Course ID : {doc_keys[k]}, Similarity Score : {query_result[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course ID : EE-727, Similarity Score : 0.9965330556538541\n",
      "Course ID : EE-593, Similarity Score : 0.9327644994447769\n",
      "Course ID : CS-486, Similarity Score : 0.5517138088494876\n",
      "Course ID : COM-308, Similarity Score : 0.414630098120584\n",
      "Course ID : MGT-401, Similarity Score : 0.4130553043559283\n"
     ]
    }
   ],
   "source": [
    "search_function([\"facebook\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course ID : MATH-332, Similarity Score : 0.8540010046589795\n",
      "Course ID : COM-516, Similarity Score : 0.8487972977682151\n",
      "Course ID : MATH-600, Similarity Score : 0.7821969940408016\n",
      "Course ID : MGT-484, Similarity Score : 0.5951955054703466\n",
      "Course ID : ME-499, Similarity Score : 0.40679820401054506\n"
     ]
    }
   ],
   "source": [
    "search_function([\"markov chain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we do a query similar to what we have done in the previous part :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course ID : EE-727, Similarity Score : 1.021507248010081\n",
      "Course ID : EE-593, Similarity Score : 0.9078726858823052\n",
      "Course ID : COM-516, Similarity Score : 0.8757003297538931\n",
      "Course ID : MATH-332, Similarity Score : 0.8528451337074313\n",
      "Course ID : MATH-600, Similarity Score : 0.7858960691559022\n"
     ]
    }
   ],
   "source": [
    "search_function([\"facebook\",\"markov chain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are treally great, better than with only the TF-IDF approach. It \"captures\" well the idea of social network and so gives enough importance to facebook so that relevent courses are more on top than before (e.g. EE-593)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Document-document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we use to compare two courses in our latent space is : cos-sim$(S\\cdot V^T_{d1},S\\cdot V^T_{d2})$, where $V^T_{di}$ indicates the i-th column vector from $V^T$, and cos-sim is the cosine similarity :\n",
    "\n",
    "cos-sim(v1,v2) = $\\frac{v1 \\cdot v2}{||v1||*||v2||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cosine similarity between two documents. Take their two ids as parameters.\n",
    "\"\"\"\n",
    "def sim_documents(d1,d2):\n",
    "    doc_1 = np.diag(S) @ V_T[:,d1]\n",
    "    doc_2 = np.diag(S) @ V_T[:,d2]\n",
    "    return (doc_1 @ doc_2)/(np.linalg.norm(doc_1)*np.linalg.norm(doc_2))\n",
    "\n",
    "\"\"\"\n",
    "Take a course and output the num courses the closest to it in the latent space (in the cosine sense)\n",
    "\"\"\"\n",
    "def course_recommender(course,num=5):\n",
    "    course_id = course_ids[course]\n",
    "    corpus_similarity = np.zeros(TFIDF.shape[1])\n",
    "    for i in range(TFIDF.shape[1]):\n",
    "        if i != course_id:\n",
    "            corpus_similarity[i] = sim_documents(i,course_id)\n",
    "    best_fit = corpus_similarity.argsort()[-num:][::-1]\n",
    "    for k in best_fit:\n",
    "        print(doc_keys[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN-525\n",
      "CS-423\n",
      "CS-401\n",
      "EE-724\n",
      "CS-322\n"
     ]
    }
   ],
   "source": [
    "course_recommender(\"COM-308\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendation are actually pretty great! (Except maybe the last one which does not exactly capture the essence of the course, but still it is an important course)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
