{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *H*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Baffou Jérémy*\n",
    "* *Basseto Antoine*\n",
    "* *Pinto Andrea*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me it makes sense to do filtering operations in this order:\n",
    "\n",
    "- remove punctuation\n",
    "- remove stop words\n",
    "- remove digits\n",
    "- lower case the words\n",
    "- stemming\n",
    "- lemming\n",
    "- compute frequencies over all corpus\n",
    "- remove too frequent and unfrequent terms \n",
    "- add n-grams (2 and 3 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_rdd = sc.parallelize(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_processed = courses_rdd.map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [word.casefold() for word in c[\"description\"].translate(str.maketrans('', '', string.punctuation)).split() if (word.lower() not in stopwords and not word.isdigit())]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_processed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also cut words with digit in it as they don't give information about course content most of the times (except 3d which maybe we could keep?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_processed = courses_processed.map(lambda c :{\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in c[\"description\"] if not any(i.isdigit() for i in w)]}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "def tag_mapper(tag):\n",
    "    if tag[0] in tag_dict:\n",
    "        return tag_dict[tag[0]]\n",
    "    else :\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize(words):\n",
    "    return list(map(lambda w : lm.lemmatize(w[0],tag_mapper(w[1])),nltk.pos_tag(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization  (have to collect dataset because can't figure how to have wordnet downloaded on every workers)\n",
    "lm = WordNetLemmatizer()\n",
    "courses_lemmatized = sc.parallelize(list(map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : lemmatize(c[\"description\"])},courses_processed.collect())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] : Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "courses_stemmed = courses_lemmatized.map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : list(map(lambda w : ps.stem(w), c[\"description\"]))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure about stemming here. I mean it can cut words and group them into bigger group (worker, and working will be map to work which isn't done by the lemmatizer here). But sometimes it cuts too much end of words, so we lose a bit of meaning.\n",
    "\n",
    "Now we're going to compute the frequencies of words in the **entire corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#courses_word_aggregation = courses_stemmed.flatMap(lambda c : c[\"description\"]) #flatten all words lists\n",
    "courses_word_aggregation = courses_lemmatized.flatMap(lambda c : c[\"description\"]) #flatten all words lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_number = courses_word_aggregation.count()\n",
    "words_count = courses_word_aggregation.map(lambda w : (w,1)).reduceByKey(lambda w1,w2 : w1+w2).map(lambda w : (w[1],w[0])).sortByKey(False)\n",
    "words_freq = words_count.map(lambda w : (w[1],w[0]/words_number))\n",
    "words_freq_for_plot = np.asarray(words_freq.map(lambda w : w[1]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(16,4))\n",
    "axs[0].set_title(\"Frequencies of words\")\n",
    "axs[0].set_xlabel(\"wordId\")\n",
    "axs[0].set_ylabel(\"log of frequencies\")\n",
    "plot1 = axs[0].plot(np.linspace(0,len(words_freq_for_plot), num=len(words_freq_for_plot)),np.log(words_freq_for_plot))\n",
    "plot2 = axs[1].boxplot(words_freq_for_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to erase words that are too frequent in the corpus as they are unlikely to differentiate documents. We choose to keep only the words which were under the 0.975 quantile because it corresponds more or less to the big vertical bar at the beginning in the frequencies plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_frequent_indices = np.where(words_freq_for_plot > np.quantile(words_freq_for_plot,0.60)) # maybe change quantile\n",
    "really_frequent_words = set(words_freq.take(really_frequent_indices[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(attention, remove << and >> in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the infrequent words we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_apparition_words = words_count.filter(lambda w : w[0] == 1)\n",
    "single_apparition_words.count()/words_count.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these words are a big part of the dataset, should we really cut them?\n",
    "\n",
    "Now we cut the really frequent words in the list of words per course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag_of_words_per_course = courses_stemmed.map(lambda c :{\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in c[\"description\"] if w not in really_frequent_words]}) \n",
    "bag_of_words_per_course = courses_lemmatized.map(lambda c :{\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in c[\"description\"] if w not in really_frequent_words]}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big question is at which step are we supposed to create the n-grams :\n",
    "- before the first processing step\n",
    "- after lemmatization\n",
    "- after stemming \n",
    "\n",
    "I think that starting with after first lemming step is a good choice because we are interested of words in their context. And stemming is mainly here to extract most of the information of a single word but we lose information about the original one which, when associated with other, can give more information.\n",
    "\n",
    "Another big question is how many n_grams do we create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after lemming and cut of frequent words:\n",
    "two_grams = bag_of_words_per_course.map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in ngrams(c[\"description\"],2)]})\n",
    "three_grams = bag_of_words_per_course.map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in ngrams(c[\"description\"],3)]})\n",
    "four_grams =bag_of_words_per_course.map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in ngrams(c[\"description\"],4)]})\n",
    "n_grams = two_grams.union(three_grams.union(four_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_aggregation = n_grams.flatMap(lambda c : c[\"description\"]) #flatten all words lists\n",
    "n_gram_number = n_gram_aggregation.count()\n",
    "n_gram_count = n_gram_aggregation.map(lambda w : (w,1)).reduceByKey(lambda w1,w2 : w1+w2).map(lambda w : (w[1],w[0])).sortByKey(False)\n",
    "n_gram_freq = n_gram_count.map(lambda w : (w[1],w[0]/n_gram_number))\n",
    "n_gram_freq_for_plot = np.asarray(n_gram_freq.map(lambda w : w[1]).collect())\n",
    "fig,axs = plt.subplots(1,2,figsize=(16,4))\n",
    "axs[0].set_title(\"Frequencies of words\")\n",
    "axs[0].set_xlabel(\"ngram Id\")\n",
    "axs[0].set_ylabel(\"log of frequencies\")\n",
    "plot1 = axs[0].plot(np.linspace(0,len(n_gram_freq_for_plot), num=len(n_gram_freq_for_plot)),np.log(n_gram_freq_for_plot))\n",
    "plot2 = axs[1].boxplot(n_gram_freq_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_frequent_n_gram_indices = np.where(n_gram_freq_for_plot > np.quantile(n_gram_freq_for_plot,0.5)) # maybe change quantile\n",
    "really_frequent_n_gram = set(n_gram_freq.take(really_frequent_n_gram_indices[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sould we cut off the n-grams that are too frequent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = n_grams.map(lambda c :{\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [w for w in c[\"description\"] if w not in really_frequent_n_gram]}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = n_grams.map(lambda c : {\"courseId\" : c[\"courseId\"], \"name\" : c[\"name\"], \"description\" : [\" \".join(w) for w in c[\"description\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the bag of words per course, put them in sets and make union so that we don't keep duplicate. Then we will do the same things for n-grams, and finally union them all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_per_course = bag_of_words_per_course.map(lambda c : (c[\"courseId\"], (c[\"name\"],c[\"description\"]))).union(n_grams.map(lambda c : (c[\"courseId\"], (c[\"name\"],c[\"description\"])))).reduceByKey(lambda a,b : (a[0],a[1]+b[1]))\n",
    "bag_of_words_per_course = bag_of_words_per_course.map(lambda c : {\"courseId\" : c[0], \"name\" : c[1][0], \"description\" : c[1][1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = bag_of_words_per_course.map(lambda c : set(c[\"description\"])).reduce(lambda a,b : a.union(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IX description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should the n_grams be included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(sorted(bag_of_words_per_course.filter(lambda c : c[\"courseId\"] == \"COM-308\").take(1)[0][\"description\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an implementation of TF-IDF which uses the term frequecy expression : $\\frac{f_{td}}{\\sum_{t' \\in d}{f_{t',d}}}$, and as inverse document frequency : $\\log{\\frac{N}{|d \\in D : t \\in d|}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_id_document_mapping = dict(zip(bag_of_words_per_course.map(lambda c : c[\"courseId\"]).collect(), range(bag_of_words_per_course.count())))\n",
    "term_id_mapping = dict(zip(sorted(bag_of_words), range(len(bag_of_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_freq_element(t):\n",
    "    term_frequency[t[0][0]][t[0][1]] = t[1]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_count = bag_of_words_per_course.map(lambda c : [((term_id_mapping[w],course_id_document_mapping[c[\"courseId\"]]),1) for w in c[\"description\"]]).flatMap(lambda c : c).reduceByKey(lambda a,b : a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = np.zeros((len(term_id_mapping),len(course_id_document_mapping)),dtype=np.int64)\n",
    "idf = np.ones((term_frequency.shape[0],1),dtype=np.int64)*term_frequency.shape[1]\n",
    "term_count = list(map(lambda t : set_freq_element(t), term_count))\n",
    "term_frequency = term_frequency/term_frequency.sum(axis=0)\n",
    "idf = idf/(np.bincount(np.where(term_frequency != 0)[0]).reshape(idf.shape[0],1))\n",
    "tf_idf = term_frequency*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IX top 15 words in term of TF-IDF score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(list(zip(tf_idf[:,course_id_document_mapping[\"COM-308\"]],term_id_mapping.keys())),key=lambda t : t[0],reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the choice of our TF-IDF score implementation, its value is higher when the term is frequent in the document but not in the rest of the corpus ($t_f$ is big and ${|d \\in D : t \\in d|}$ is small thus idf is big)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(d_i,d_j):\n",
    "    return (d_i.T@d_j)/(np.linalg.norm(d_i)*np.linalg.norm(d_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = np.zeros((tf_idf.shape[0],1),dtype=np.int64)\n",
    "query_term = [\"markov chain\",\"facebook\"]\n",
    "for t in query_term:\n",
    "    query_doc[term_id_mapping[t]] = np.max(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = np.zeros((tf_idf.shape[1],1))\n",
    "for i in range(tf_idf.shape[1]):\n",
    "    query_result[i,0] = cos_similarity(query_doc,tf_idf[:,i].reshape(tf_idf.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip(query_result,course_id_document_mapping.keys())),key=lambda t : t[0],reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processor(words_list,num=5):\n",
    "    query_doc = np.zeros((tf_idf.shape[0],1),dtype=np.int64)\n",
    "    for t in words_list:\n",
    "        query_doc[term_id_mapping[t]] = np.max(tf_idf)\n",
    "    query_result = np.zeros((tf_idf.shape[1],1))\n",
    "    for i in range(tf_idf.shape[1]):\n",
    "        query_result[i,0] = cos_similarity(query_doc,tf_idf[:,i].reshape(tf_idf.shape[0],1))\n",
    "    best_fit = sorted(list(zip(query_result,course_id_document_mapping.keys())),key=lambda t : t[0],reverse=True)[:num]\n",
    "    courses_description = courses_rdd.filter(lambda c : c[\"courseId\"] in list(map(lambda t : t[1],best_fit))).collect() \n",
    "    output = []\n",
    "    for i in best_fit:\n",
    "        for j in courses_description: ###  A UPGRADE!!!!!\n",
    "            if i[1] == j[\"courseId\"]:\n",
    "                output.append(j)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_processor([\"markov chain\",\"facebook\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
