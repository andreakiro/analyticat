{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 4: Word2Vec\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *H*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Antoine Basseto*\n",
    "* *Andrea Pinto*\n",
    "* *Jérémy Baffou*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 4 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "from utils import *\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess text description\n",
    "@param: (description) Dirty string text\n",
    "@return: Cleaned array of words\n",
    "\"\"\"\n",
    "def preprocess(description):\n",
    "    # Preprocess courses descriptions\n",
    "    split_joined_words = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', description)\n",
    "    split_3D_occurences = re.sub(r'3D', r' 3D ', split_joined_words)\n",
    "    stick_phd_occurences = re.sub(r'Ph D', r'PhD', split_3D_occurences)\n",
    "    cleaned_dataset = stick_phd_occurences\n",
    "    \n",
    "    # Remove punctuations and separate\n",
    "    cleaned_dataset = (cleaned_dataset\n",
    "                       .translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "                       .split())\n",
    "    \n",
    "    # Remove stopwords and digits\n",
    "    cleaned_dataset = [\n",
    "        word for word in cleaned_dataset \n",
    "        if word.lower() not in stopwords and not word.isdigit()\n",
    "    ]\n",
    "    \n",
    "    return cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Clean courses RDD\n",
    "@param: (courses_rdd) Dirty courses RDD\n",
    "@return: Cleaned courses RDD\n",
    "\"\"\"\n",
    "def clean_rdd(courses_rdd):\n",
    "    return courses_rdd.map(\n",
    "        lambda c: {\n",
    "            'courseId': c['courseId'],\n",
    "            'name': c['name'],\n",
    "            'description': preprocess(c['description'])\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_rdd = sc.parallelize(courses)\n",
    "courses_preprocessed_rdd = clean_rdd(courses_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Frequency analysis of given corpus\n",
    "@param: (corpus) RDD of list of words\n",
    "@return: RDD of list of (frequency ratio, word) tuples\n",
    "@return: Number of words in the corpus\n",
    "\"\"\"\n",
    "def frequency_analysis(corpus):\n",
    "    count = corpus.count()\n",
    "    freqs2word_rdd = (\n",
    "        corpus\n",
    "        .map(lambda word: (word, 1))\n",
    "        .reduceByKey(lambda x, y: x + y)\n",
    "        .map(lambda x: (x[1] / count, x[0]))\n",
    "        .sortByKey(False)\n",
    "    )\n",
    "    return freqs2word_rdd, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = courses_preprocessed_rdd.flatMap(lambda c: c[\"description\"])\n",
    "freqs2word_rdd, count = frequency_analysis(corpus)\n",
    "freqs = np.asarray(freqs2word_rdd.map(lambda x: x[0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(freqs2word_rdd, freqs, quantile):\n",
    "    indices = np.where(freqs > np.quantile(freqs, quantile))[0]\n",
    "    frequent = freqs2word_rdd.take(indices[-1])\n",
    "    frequent_words = set(map(lambda x: x[1], frequent))\n",
    "    return frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_words(freqs2word_rdd, count, apparitions):\n",
    "    rare = freqs2word_rdd.filter(lambda x: x[0]*count == apparitions)\n",
    "    rare_words = set(rare.map(lambda x: x[1]).collect())\n",
    "    return rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rdd(courses_rdd, words):\n",
    "    return courses_rdd.map(\n",
    "        lambda c: {\n",
    "            \"courseId\": c[\"courseId\"],\n",
    "            \"name\": c[\"name\"],\n",
    "            \"description\": [w for w in c[\"description\"] if w not in words]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = frequent_words(freqs2word_rdd, freqs, 0.995)\n",
    "rw = rare_words(freqs2word_rdd, count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 of frequent words out of 16772 total words.\n",
      "7757 of rare words out of 16772 total words.\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(fw)} of frequent words out of {freqs2word_rdd.count()} total words.')\n",
    "print(f'{len(rw)} of rare words out of {freqs2word_rdd.count()} total words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only delete frequent words, as words that occur even only once account for 46% of the data, which is too much to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_rdd = remove_rdd(courses_preprocessed_rdd, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.12 : Clustering word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('/ix/model.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(model, word):\n",
    "    # Get a default word_vector for words out of the vocabulary of the model, \n",
    "    # use the zero vector so it is not really taken into account for the calculation of\n",
    "    # document and query vector representations\n",
    "    return model.get_vector(word) if word in model.vocab else np.zeros(len(model.get_vector(\"dog\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w2v_model.get_vector(\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that vectors in our model are already normalised, meaning that k-means, even though it is using the euclidean distance and not the cosine similarity, should find reasonable clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get dict with words in the data and their vector \n",
    "representation according to the given model, for use with the kmeans algorithm.\n",
    "@param: (data_rdd) RDD of list of words\n",
    "@param: (model) Gensim w2v model\n",
    "@return: dict of words in the data and their vector representation\n",
    "\"\"\"\n",
    "def get_data_word_vectors(data_rdd, model):\n",
    "    data_w2v = {} \n",
    "    for word in data_rdd.flatMap(lambda c : c[\"description\"]).collect():\n",
    "        # As this is for use with the kmeans algorithm, we discard any data not in \n",
    "        # the model as it would create a big cluster around the default word vector \n",
    "        # and pollute our findings\n",
    "        if word in model.vocab:\n",
    "            data_w2v.update({word: get_word_vector(model, word)})\n",
    "        \n",
    "    return data_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_w2v = get_data_word_vectors(preprocessed_data_rdd, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means to cluster word vectors of the unique (guaranteed by the list of dictionnary values)\n",
    "# words in our pre-processed dataset\n",
    "kmeans = KMeans(n_clusters=25, random_state=0).fit(list(data_w2v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = kmeans.predict(list(data_w2v.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_n_cluster(kmeans, data_w2v, model, topn=10):    \n",
    "    data_vectors = np.array(list(data_w2v.values()))\n",
    "    data_word = np.array(list(data_w2v.keys()))\n",
    "    \n",
    "    for i, center in enumerate(kmeans.cluster_centers_):\n",
    "        similarities = model.cosine_similarities(center, data_vectors[cluster_assignments == i])\n",
    "        # get the closest words in ascending order\n",
    "        top_indices = np.argsort(similarities)[:-topn-1:-1]\n",
    "        print(f'The {topn} closest words to cluster center #{i} are:\\n{data_word[cluster_assignments == i][top_indices]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 closest words to cluster center #0 are:\n",
      "['consecutive' 'final' 'starting' 'September' 'scoring' 'August' 'ended'\n",
      " 'June' 'July' 'October']\n",
      "\n",
      "The 10 closest words to cluster center #1 are:\n",
      "['optimal' 'parameters' 'optimizing' 'evaluating' 'optimum' 'determining'\n",
      " 'utilization' 'flexibility' 'robustness' 'tradeoffs']\n",
      "\n",
      "The 10 closest words to cluster center #2 are:\n",
      "['Perturbation' 'Variational' 'Estimation' 'Processes' 'Analysis'\n",
      " 'Iterative' 'Kinematics' 'Function' 'Deterministic' 'Mechanisms']\n",
      "\n",
      "The 10 closest words to cluster center #3 are:\n",
      "['seminars' 'teaching' 'academics' 'internships' 'lecturers' 'education'\n",
      " 'faculty' 'coursework' 'educational' 'professors']\n",
      "\n",
      "The 10 closest words to cluster center #4 are:\n",
      "['notion' 'notions' 'epistemologies' 'normative' 'context' 'rationality'\n",
      " 'subjectivity' 'worldview' 'understanding' 'paradigm']\n",
      "\n",
      "The 10 closest words to cluster center #5 are:\n",
      "['connecting' 'adjacent' 'linking' 'junction' 'situated' 'stretching'\n",
      " 'area' 'connected' 'located' 'areas']\n",
      "\n",
      "The 10 closest words to cluster center #6 are:\n",
      "['Automation' 'Systems' 'Technologies' 'Wideband' 'Solutions'\n",
      " 'Microprocessor' 'Affymetrix' 'Devices' 'Parametric' 'Integrator']\n",
      "\n",
      "The 10 closest words to cluster center #7 are:\n",
      "['Make' 'Mind' 'Shape' 'Perfect' 'Noise' 'Hands' 'Trouble' 'Face'\n",
      " 'Imagination' 'Memories']\n",
      "\n",
      "The 10 closest words to cluster center #8 are:\n",
      "['photodetectors' 'photoluminescence' 'photocurrent' 'nonlinearities'\n",
      " 'photoemission' 'photomultipliers' 'excitation' 'linewidth' 'dielectric'\n",
      " 'optical']\n",
      "\n",
      "The 10 closest words to cluster center #9 are:\n",
      "['chimie' 'illustré' 'Mathématiques' 'été' 'édition' 'mémoire' 'Librairie'\n",
      " 'toute' 'dans' 'Universitaires']\n",
      "\n",
      "The 10 closest words to cluster center #10 are:\n",
      "['Thompson' 'Smith' 'Moore' 'Baker' 'Patterson' 'Taylor' 'Richardson'\n",
      " 'Allen' 'Robinson' 'Freeman']\n",
      "\n",
      "The 10 closest words to cluster center #11 are:\n",
      "['instability' 'disruption' 'resulting' 'irreversible' 'fluctuations'\n",
      " 'gradual' 'result' 'inevitably' 'slowing' 'accumulation']\n",
      "\n",
      "The 10 closest words to cluster center #12 are:\n",
      "['thin' 'cylindrical' 'casing' 'vertical' 'horizontal' 'surfaces' 'wires'\n",
      " 'mesh' 'concave' 'sliding']\n",
      "\n",
      "The 10 closest words to cluster center #13 are:\n",
      "['groundwater' 'sediment' 'contaminants' 'wastewaters' 'biomass' 'water'\n",
      " 'evaporation' 'soils' 'vegetation' 'aquifer']\n",
      "\n",
      "The 10 closest words to cluster center #14 are:\n",
      "['investments' 'investment' 'investing' 'pricing' 'investors' 'firms'\n",
      " 'securities' 'enterprises' 'cashflow' 'suppliers']\n",
      "\n",
      "The 10 closest words to cluster center #15 are:\n",
      "['illustrative' 'intricate' 'interesting' 'expressive' 'descriptions'\n",
      " 'familiar' 'characteristic' 'describing' 'illustrate' 'texts']\n",
      "\n",
      "The 10 closest words to cluster center #16 are:\n",
      "['software' 'interfaces' 'implementations' 'frontend' 'APIs'\n",
      " 'functionality' 'interface' 'Simulink' 'GUI' 'OLTP']\n",
      "\n",
      "The 10 closest words to cluster center #17 are:\n",
      "['understand' 'analyze' 'demonstrate' 'identify' 'bring' 'integrate'\n",
      " 'formulate' 'examine' 'explain' 'communicate']\n",
      "\n",
      "The 10 closest words to cluster center #18 are:\n",
      "['polyol' 'polymers' 'azides' 'thiols' 'carbonylation' 'silane'\n",
      " 'polymeric' 'compounds' 'nitrides' 'isomerisation']\n",
      "\n",
      "The 10 closest words to cluster center #19 are:\n",
      "['apoptosis' 'misfolding' 'neuronal' 'proteins' 'hematopoiesis'\n",
      " 'inhibition' 'intracellular' 'epithelial' 'neuroprotection'\n",
      " 'angiogenesis']\n",
      "\n",
      "The 10 closest words to cluster center #20 are:\n",
      "['Werner' 'Schürmann' 'Georg' 'Hans' 'Bernhard' 'Schmid' 'Emil' 'Meyer'\n",
      " 'Philipp' 'Moser']\n",
      "\n",
      "The 10 closest words to cluster center #21 are:\n",
      "['Biochemistry' 'Microbiology' 'Neuroscience' 'Biophysics' 'Biomedical'\n",
      " 'Genetics' 'Immunology' 'Pharmacology' 'Molecular' 'Neurophysiology']\n",
      "\n",
      "The 10 closest words to cluster center #22 are:\n",
      "['quadratic' 'ODEs' 'eigenvalues' 'invariant' 'finite' 'summability'\n",
      " 'equations' 'discretized' 'integrability' 'invariants']\n",
      "\n",
      "The 10 closest words to cluster center #23 are:\n",
      "['knowing' 'explaining' 'informed' 'arguing' 'questioning' 'understands'\n",
      " 'concerned' 'insist' 'realizing' 'aware']\n",
      "\n",
      "The 10 closest words to cluster center #24 are:\n",
      "['Resource' 'Implementation' 'Policies' 'Policy' 'Governance' 'Evaluation'\n",
      " 'Innovation' 'Information' 'Budgeting' 'Strategy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_n_cluster(kmeans, data_w2v, w2v_model, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe clusters about course topics, academics, actions, people's names, concepts (time) or french words.\n",
    "\n",
    "We give the following labels for 10 of the above clusters:\n",
    "\n",
    "concepts\n",
    "- cluster 0 is `time`\n",
    "- cluster 3 is `education`\n",
    "\n",
    "french words\n",
    "- cluster 9 is `french words`\n",
    "\n",
    "names\n",
    "- cluster 10 is `names`\n",
    "\n",
    "actions\n",
    "- cluster 17 is `understanding`\n",
    "\n",
    "course topics`\n",
    "- cluster 14 is `finance`\n",
    "- cluster 16 is `developement`\n",
    "- cluster 18 is `chemistry`\n",
    "- cluster 19 is `life sciences`\n",
    "- cluster 22 is `mathematics`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are much more meaningful here than for LSI and LDA, almost all could have a clear meaning associated to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.13 : Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DF(data_rdd):\n",
    "    DF = {}\n",
    "    for word in data_rdd.flatMap(lambda c : c[\"description\"]).collect():\n",
    "        if word not in DF:\n",
    "            DF[word] = 1\n",
    "        else:\n",
    "            DF[word] += 1\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = get_DF(preprocessed_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get courses with a vector for their description\n",
    "@param: (data_rdd) RDD of the data\n",
    "@return: list of dict of courses with a vector component\n",
    "\"\"\"\n",
    "def document_vectors_rdd(data_rdd, model, DF):\n",
    "    return list(map(lambda c: add_vector_to_course(c, model, DF), data_rdd.collect()))\n",
    "\n",
    "def add_vector_to_course(course, model, DF):\n",
    "    TF_IDF = {word: TF/DF[word] for word, TF in Counter(course['description']).items()}\n",
    "    total_TF_IDF = np.sum(list(TF_IDF.values()))\n",
    "    return {'courseId': course['courseId'],\n",
    "            'name': course['name'],\n",
    "            'description': course['description'],\n",
    "            'vector': np.sum([get_word_vector(model, word) * TF_IDF[word] / total_TF_IDF \n",
    "                              for word in course['description']], axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_vector(query, DF, model):\n",
    "    words = query.split()\n",
    "    if np.all([word in DF for word in words]):\n",
    "        TF_IDF = {word: TF/DF[word] for word, TF in Counter(words).items()}\n",
    "        total_TF_IDF = np.sum(list(TF_IDF.values()))\n",
    "\n",
    "        return np.sum([get_word_vector(model, word) * TF_IDF[word] / total_TF_IDF for word in words], axis=0)\n",
    "    else:\n",
    "        return np.mean([get_word_vector(model, word) for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topn_courses(courses_with_vectors, search_word_vector, model, topn=5):\n",
    "    result = list(map(lambda c: (c, model.cosine_similarities(search_word_vector, [c['vector']])[0]),\n",
    "                      courses_with_vectors))\n",
    "    result.sort(reverse=True, key=lambda t: t[1])\n",
    "    return result[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topn_courses(courses_with_vectors, query, DF, model, topn=5):\n",
    "    search_word_vector = get_query_vector(query, DF, model)\n",
    "    sorted_courses = get_topn_courses(courses_with_vectors, search_word_vector, model, topn=topn)\n",
    "    \n",
    "    print(f'Searching the top {topn} courses for query \\\"{query}\\\":\\n')\n",
    "    for i, c in enumerate(sorted_courses):\n",
    "        print(f'Result #{i} is the course {c[0][\"courseId\"]} {c[0][\"name\"]} with similarity {c[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_with_vectors = document_vectors_rdd(preprocessed_data_rdd, w2v_model, DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the top 5 courses for query \"Markov chains\":\n",
      "\n",
      "Result #0 is the course MATH-332 Applied stochastic processes with similarity 0.5617865746406503\n",
      "Result #1 is the course MGT-484 Applied probability & stochastic processes with similarity 0.5294180894619351\n",
      "Result #2 is the course COM-516 Markov chains and algorithmic applications with similarity 0.5192955703253681\n",
      "Result #3 is the course CH-311 Molecular and cellular biophysic I with similarity 0.4822200434776796\n",
      "Result #4 is the course MSE-211 Organic chemistry with similarity 0.472022200454137\n"
     ]
    }
   ],
   "source": [
    "print_topn_courses(courses_with_vectors, \"Markov chains\", DF, w2v_model, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the top 5 courses for query \"Facebook\":\n",
      "\n",
      "Result #0 is the course EE-727 Computational Social Media with similarity 0.7550354730039651\n",
      "Result #1 is the course COM-308 Internet analytics with similarity 0.4954955788676604\n",
      "Result #2 is the course CS-622 Privacy Protection with similarity 0.4782632547353867\n",
      "Result #3 is the course COM-208 Computer networks with similarity 0.4726744392633931\n",
      "Result #4 is the course CS-486 Human computer interaction with similarity 0.47204086049787874\n"
     ]
    }
   ],
   "source": [
    "print_topn_courses(courses_with_vectors, \"Facebook\", DF, w2v_model, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are quite good, two courses in the `Markov chains` query are more related to carbon chains than anythong else but we can still conclude that overall the results are comparable to LSI and better than using only the TF-IDF (especially when considering the `Facebook` results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.14: Document similarity search with outside terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the top 5 courses for query \"MySpace Orkut\":\n",
      "\n",
      "Result #0 is the course EE-727 Computational Social Media with similarity 0.7118328548684894\n",
      "Result #1 is the course COM-208 Computer networks with similarity 0.5225126718946227\n",
      "Result #2 is the course COM-308 Internet analytics with similarity 0.5213876867320217\n",
      "Result #3 is the course MGT-517 Entrepreneurship laboratory (e-lab) with similarity 0.48860631331937526\n",
      "Result #4 is the course CS-486 Human computer interaction with similarity 0.485148847320839\n"
     ]
    }
   ],
   "source": [
    "print_topn_courses(courses_with_vectors, \"MySpace Orkut\", DF, w2v_model, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are almost the same as for the `Facebook` query, which makes sense and supports the validity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the top 5 courses for query \"coronavirus\":\n",
      "\n",
      "Result #0 is the course BIO-657 Landmark Papers in Cancer and Infection with similarity 0.5979577792035651\n",
      "Result #1 is the course BIO-477 Infection biology with similarity 0.5882584923093604\n",
      "Result #2 is the course BIO-638 Practical - Lemaitre Lab with similarity 0.5714327124173664\n",
      "Result #3 is the course CH-414 Pharmacological chemistry with similarity 0.5480594849117834\n",
      "Result #4 is the course BIOENG-433 Biotechnology lab (for CGC) with similarity 0.5412282985903536\n"
     ]
    }
   ],
   "source": [
    "print_topn_courses(courses_with_vectors, \"coronavirus\", DF, w2v_model, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results here are quite satisfactory, with all courses pertaining to life sciences and the two most similar to infections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
