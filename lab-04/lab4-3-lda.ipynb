{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *H*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Jérémy Baffou*\n",
    "* *Antoine Basseto*\n",
    "* *Andrea Pinto*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from utils import load_json, load_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark.mlli` useful docs for the lab : \n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.clustering.LDA.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.clustering.LDAModel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data from lab4-1-vsm with all lemmatized terms \n",
    "courses_preprocessed = load_json('data/courses-lem-preprocessed.json')\n",
    "courses_preprocessed_rdd = sc.parallelize(courses_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data from lab4-1-vsm without most frequent words (quantile 0.95)\n",
    "courses_preprocessed_fw = load_json('data/courses-lem-fw-preprocessed.json')\n",
    "courses_preprocessed_fw_rdd = sc.parallelize(courses_preprocessed_fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We tried with both datasets and the `courses_preprocessed` gives way better results. <br>\n",
    "So all the following computations are done with this latter and we'll not use `courses_preprocessed_fw`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Data for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create super set bag of words\n",
    "@param: (rdd) RDD dataset of courses\n",
    "@return: Set of all existing terms\n",
    "\"\"\"\n",
    "def bow(rdd):\n",
    "    return set(rdd.flatMap(lambda c: c[\"description\"]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Populate term count matrix\n",
    "@param: (data) Term counts in a list format i.e. tuples (a, b)\n",
    "with a: tuple (doc ID, term ID) and b: number of occurences\n",
    "@param: (m) Term count matrix rows i.e. number of documents\n",
    "@param: (n) Term count matrix columns i.e. number of documents\n",
    "@return: Full term count matrix\n",
    "\"\"\"\n",
    "def populate_tc(data, n, m):\n",
    "    tc = np.zeros((n, m))\n",
    "    for x in data:\n",
    "        infos = x[0]\n",
    "        value = x[1]\n",
    "        \n",
    "        doc_id = infos[0]\n",
    "        word_id = infos[1]\n",
    "        \n",
    "        tc[doc_id][word_id] = value\n",
    "    \n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create Sparse Vector from Dense Vector\n",
    "@param: (dense_vect) Dense Vector input\n",
    "@return: Same vector under Sparse Vector format\n",
    "\"\"\"\n",
    "def create_sparse_vector(dense_vect):\n",
    "    d = len(dense_vect)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for i, e in enumerate(dense_vect):\n",
    "        if e != 0:\n",
    "            indices.append(i)\n",
    "            values.append(e)\n",
    "    return Vectors.sparse(d, indices, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful following variables documentation. Note that none of these variables are RDDs.\n",
    "- **`superbow`** is the *set* of every existing terms we are dealing with.\n",
    "- **`words_ids_map`** is a *dict* mapping every existing terms to a specific ID $\\in N$\n",
    "- **`doc_ids_map`** is a *dict* mapping every existing documents (here courses descriptions) to a specific ID $\\in N$\n",
    "- **`tc`** is the term count *matrix*. It has one row for every document and one column for every existing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "superbow = bow(courses_preprocessed_rdd)\n",
    "words_ids_map = dict(zip(superbow, range(len(superbow))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_ids = courses_preprocessed_rdd.map(lambda c: c['courseId']).collect()\n",
    "doc_ids_map = dict(zip(courses_ids, range(len(courses_ids))))\n",
    "doc_ids = list(doc_ids_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount = (\n",
    "    courses_preprocessed_rdd\n",
    "    .map(lambda c: [((doc_ids_map[c[\"courseId\"]], words_ids_map[w]),1) for w in c[\"description\"]])\n",
    "    .flatMap(lambda c: c)\n",
    "    .reduceByKey(lambda x, y : x + y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = populate_tc(termcount.collect(), len(doc_ids), len(superbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Model** requires **`doc_tc`** a list of tuples with documents IDs and corresponding terms count under a `pyspark.mllib.linalg.vector` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_dense = [Vectors.dense(wordcount) for wordcount in tc]\n",
    "tcs_sparse = list(map(lambda x: create_sparse_vector(x), tcs_dense))\n",
    "doc_tc_tup = list(zip(doc_ids, tcs_sparse))\n",
    "doc_tc = list(map(lambda x: list(x), doc_tc_tup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper cell can take 1-2s to run because we cast to Sparse Vectors format for Model optimization. <br>\n",
    "Note also that in reality the LDA model needs this funny `doc_tc` list but in `RDD` format : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(doc_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract K topics from LDA Model\n",
    "@param: (model) LDA Model on which we work\n",
    "@param: (words_per_topic) Words to print per topic\n",
    "@param: (words_ids_map) Dict mapping words to their ID\n",
    "\"\"\"\n",
    "def extract_topic(model, words_per_topic, words_ids_map):\n",
    "    topics = model.describeTopics(words_per_topic)\n",
    "    for i, topic in enumerate(topics):\n",
    "        words_ids = topic[0]\n",
    "        words = []\n",
    "        for word_id in words_ids:\n",
    "            word = list(words_ids_map.keys())[word_id]\n",
    "            words.append(word)\n",
    "        print(f'For topic {i + 1 :>2} we have: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model and topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_48 = LDA.train(rdd, k=10, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic  1 we have: ['image', 'method', 'signal', 'system', 'process']\n",
      "For topic  2 we have: ['data', 'student', 'method', 'report', 'analysis']\n",
      "For topic  3 we have: ['material', 'structure', 'method', 'student', 'learn']\n",
      "For topic  4 we have: ['student', 'project', 'work', 'system', 'design']\n",
      "For topic  5 we have: ['model', 'method', 'theory', 'time', 'problem']\n",
      "For topic  6 we have: ['learn', 'model', 'student', 'method', 'teach']\n",
      "For topic  7 we have: ['student', 'method', 'optic', 'engineering', 'learn']\n",
      "For topic  8 we have: ['student', 'cell', 'chemical', 'biology', 'molecular']\n",
      "For topic  9 we have: ['system', 'design', 'circuit', 'device', 'technology']\n",
      "For topic 10 we have: ['energy', 'student', 'learn', 'method', 'content']\n"
     ]
    }
   ],
   "source": [
    "extract_topic(model_48, 5, words_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics\n",
    "***TODO.***\n",
    "- Topic 1 could be `Image Processing`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with LSI\n",
    "***TODO.***\n",
    "- Proutinos le déglingo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create LDA Model with alpha variations\n",
    "@param: (rdd) RDD on which to train the model\n",
    "@param: (beta) Fixed beta (topic concentration) param\n",
    "@return: models - LDA Models trained with moving alpha param\n",
    "@return: alphas - Corresponding alpha's parameters\n",
    "\"\"\"\n",
    "def moving_alpha(rdd, beta):\n",
    "    models = []\n",
    "    alphas = []\n",
    "    for alpha in range(2, 10):\n",
    "        print(f'alpha: {alpha} beta: {beta}', end='\\r')\n",
    "        m = LDA.train(rdd, k=10, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)\n",
    "        models.append(m)\n",
    "        alphas.append(alpha)\n",
    "    print(f'Finished to train the {len(models)} models', end='\\r')\n",
    "    return models, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create LDA Model with beta variations\n",
    "@param: (rdd) RDD on which to train the model\n",
    "@param: (alpha) Fixed alpha (doc concentration) param\n",
    "@return: models - LDA Models trained with moving beta param\n",
    "@return: betas - Corresponding beta's parameters\n",
    "\"\"\"\n",
    "def moving_beta(rdd, alpha):\n",
    "    models = []\n",
    "    betas = []\n",
    "    for beta in range(1, 10):\n",
    "        beta = (beta / 10) + 1\n",
    "        print(f'alpha: {alpha} beta: {beta}', end='\\r')\n",
    "        m = LDA.train(rdd, k=10, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)\n",
    "        models.append(m)\n",
    "        betas.append(beta)\n",
    "    print(f'Finished to train the {len(models)} models', end='\\r')\n",
    "    return models, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished to train the 8 models\r"
     ]
    }
   ],
   "source": [
    "models_49a, alphas = moving_alpha(rdd, 1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished to train the 9 models\r"
     ]
    }
   ],
   "source": [
    "models_49b, betas = moving_beta(rdd, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "***TODO.***\n",
    "- Bonsoir on peut voir que proutinos est champion du monde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exercise `4.9` we found out that parameters that give the most interpretable results are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "alpha = 5\n",
    "beta = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO.*** We chose these values because el proutor is a boss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_410 = LDA.train(rdd, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic  1 we have: ['image', 'method', 'process', 'electron', 'microscopy']\n",
      "For topic  2 we have: ['student', 'data', 'project', 'report', 'method']\n",
      "For topic  3 we have: ['material', 'structure', 'method', 'student', 'learn']\n",
      "For topic  4 we have: ['student', 'model', 'learn', 'method', 'system']\n",
      "For topic  5 we have: ['model', 'method', 'theory', 'basic', 'time']\n",
      "For topic  6 we have: ['learn', 'method', 'student', 'model', 'analysis']\n",
      "For topic  7 we have: ['method', 'learn', 'student', 'concept', 'exercise']\n",
      "For topic  8 we have: ['student', 'cell', 'biology', 'molecular', 'chemical']\n",
      "For topic  9 we have: ['system', 'design', 'circuit', 'device', 'student']\n",
      "For topic 10 we have: ['energy', 'student', 'learn', 'method', 'technology']\n"
     ]
    }
   ],
   "source": [
    "extract_topic(model_410, 5, words_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics\n",
    "\n",
    "- Topic 1 `prout-prout`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_rdd = sc.textFile('/ix/wikipedia-for-schools.txt').map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create super set bag of words\n",
    "@param: (rdd) RDD dataset of courses\n",
    "@return: Set of all existing terms\n",
    "\"\"\"\n",
    "def bow_wiki(rdd):\n",
    "    return set(rdd.flatMap(lambda c: c[\"tokens\"]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful following variables documentation. Note that none of these variables are RDDs.\n",
    "- **`superbow`** is the *set* of every existing terms we are dealing with.\n",
    "- **`words_ids_map`** is a *dict* mapping every existing terms to a specific ID $\\in N$\n",
    "- **`doc_ids_map`** is a *dict* mapping every existing documents (here courses descriptions) to a specific ID $\\in N$\n",
    "- **`tc`** is the term count *matrix*. It has one row for every document and one column for every existing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_superbow = bow_wiki(wikipedia_rdd)\n",
    "tokens_ids_map = dict(zip(wiki_superbow, range(len(wiki_superbow))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ids = wikipedia_rdd.map(lambda c: c['page_id']).collect()\n",
    "wiki_doc_ids_map = dict(zip(page_ids, range(len(page_ids))))\n",
    "wiki_doc_ids = list(wiki_doc_ids_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_termcount = (\n",
    "    wikipedia_rdd\n",
    "    .map(lambda c: [((wiki_doc_ids_map[c[\"page_id\"]], tokens_ids_map[w]),1) for w in c[\"tokens\"]])\n",
    "    .flatMap(lambda c: c)\n",
    "    .reduceByKey(lambda x, y : x + y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tc = populate_tc(wiki_termcount.collect(), len(wiki_doc_ids), len(wiki_superbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Model** requires **`doc_tc`** a list of tuples with documents IDs and corresponding terms count under a `pyspark.mllib.linalg.vector` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tcs_dense = [Vectors.dense(wordcount) for wordcount in tc]\n",
    "wiki_tcs_sparse = list(map(lambda x: create_sparse_vector(x), wiki_tcs_dense))\n",
    "wiki_doc_tc_tup = list(zip(doc_ids, wiki_tcs_sparse))\n",
    "wiki_doc_tc = list(map(lambda x: list(x), wiki_doc_tc_tup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper cell can take 1-2s to run because we cast to Sparse Vectors format for Model optimization. <br>\n",
    "Note also that in reality the LDA model needs this funny `doc_tc` list but in `RDD` format : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_rdd = sc.parallelize(wiki_doc_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our interpretations we think that optimal parameters giving most interpretable results are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ?\n",
    "alpha = ?\n",
    "beta = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO.*** Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_411 = LDA.train(wiki_rdd, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_topic(model_410, 5, tokens_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics\n",
    "\n",
    "- Topic 1 `prout-prout`\n",
    "- etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
