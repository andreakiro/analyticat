{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 3: Latent Dirichlet allocation\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *H*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Jérémy Baffou*\n",
    "* *Antoine Basseto*\n",
    "* *Andrea Pinto*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from utils import load_json, load_pkl\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors, SparseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyspark.mlli` useful docs for the lab : \n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.clustering.LDA.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.clustering.LDAModel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data from lab4-1-vsm with all lemmatized terms \n",
    "courses_preprocessed = load_json('data/courses-lem-preprocessed.json')\n",
    "courses_preprocessed_rdd = sc.parallelize(courses_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data from lab4-1-vsm without most frequent words (quantile 0.95)\n",
    "courses_preprocessed_fw = load_json('data/courses-lem-fw-preprocessed.json')\n",
    "courses_preprocessed_fw_rdd = sc.parallelize(courses_preprocessed_fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We tried with both datasets and the `courses_preprocessed` gives way better results. <br>\n",
    "So all the following computations are done with this latter and we'll not use `courses_preprocessed_fw`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Data for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create super set bag of words\n",
    "@param: (rdd) RDD dataset of courses\n",
    "@return: Set of all existing terms\n",
    "\"\"\"\n",
    "def bow(rdd):\n",
    "    return set(rdd.flatMap(lambda c: c[\"description\"]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Populate term count matrix\n",
    "@param: (data) Term counts in a list format i.e. tuples (a, b)\n",
    "with a: tuple (doc ID, term ID) and b: number of occurences\n",
    "@param: (m) Term count matrix rows i.e. number of documents\n",
    "@param: (n) Term count matrix columns i.e. number of documents\n",
    "@return: Full term count matrix\n",
    "\"\"\"\n",
    "def populate_tc(data, n, m):\n",
    "    tc = np.zeros((n, m))\n",
    "    for x in data:\n",
    "        infos = x[0]\n",
    "        value = x[1]\n",
    "        \n",
    "        doc_id = infos[0]\n",
    "        word_id = infos[1]\n",
    "        \n",
    "        tc[doc_id][word_id] = value\n",
    "    \n",
    "    return tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create Sparse Vector from Dense Vector\n",
    "@param: (dense_vect) Dense Vector input\n",
    "@return: Same vector under Sparse Vector format\n",
    "\"\"\"\n",
    "def create_sparse_vector(dense_vect):\n",
    "    d = len(dense_vect)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for i, e in enumerate(dense_vect):\n",
    "        if e != 0:\n",
    "            indices.append(i)\n",
    "            values.append(e)\n",
    "    return Vectors.sparse(d, indices, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful following variables documentation. Note that none of these variables are RDDs.\n",
    "- **`superbow`** is the *set* of every existing terms we are dealing with.\n",
    "- **`words_ids_map`** is a *dict* mapping every existing terms to a specific ID $\\in N$\n",
    "- **`doc_ids_map`** is a *dict* mapping every existing documents (here courses descriptions) to a specific ID $\\in N$\n",
    "- **`tc`** is the term count *matrix*. It has one row for every document and one column for every existing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "superbow = bow(courses_preprocessed_rdd)\n",
    "words_ids_map = dict(zip(superbow, range(len(superbow))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_ids = courses_preprocessed_rdd.map(lambda c: c['courseId']).collect()\n",
    "doc_ids_map = dict(zip(courses_ids, range(len(courses_ids))))\n",
    "doc_ids = list(doc_ids_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount = (\n",
    "    courses_preprocessed_rdd\n",
    "    .map(lambda c: [((doc_ids_map[c[\"courseId\"]], words_ids_map[w]),1) for w in c[\"description\"]])\n",
    "    .flatMap(lambda c: c)\n",
    "    .reduceByKey(lambda x, y : x + y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = populate_tc(termcount.collect(), len(doc_ids), len(superbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Model** requires **`doc_tc`** a list of tuples with documents IDs and corresponding terms count under a `pyspark.mllib.linalg.vector` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_dense = [Vectors.dense(wordcount) for wordcount in tc]\n",
    "doc_tc_tup = list(zip(doc_ids, tcs_dense))\n",
    "doc_tc = list(map(lambda x: list(x), doc_tc_tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(doc_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.8: Topics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract K topics from LDA Model\n",
    "@param: (model) LDA Model on which we work\n",
    "@param: (words_per_topic) Words to print per topic\n",
    "@param: (words_ids_map) Dict mapping words to their ID\n",
    "\"\"\"\n",
    "def extract_topic(model, words_per_topic, words_ids_map):\n",
    "    topics = model.describeTopics(words_per_topic)\n",
    "    for i, topic in enumerate(topics):\n",
    "        words_ids = topic[0]\n",
    "        words = []\n",
    "        for word_id in words_ids:\n",
    "            word = list(words_ids_map.keys())[word_id]\n",
    "            words.append(word)\n",
    "        print(f'For topic {i + 1 :>2} we have: {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model and topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_48 = LDA.train(rdd, k=10, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic  1 we have: ['material', 'energy', 'property', 'process', 'chemical']\n",
      "For topic  2 we have: ['student', 'cell', 'method', 'molecular', 'learn']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'processing', 'analysis']\n",
      "For topic  4 we have: ['model', 'student', 'learn', 'method', 'exam']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'paper', 'learn', 'development']\n",
      "For topic  7 we have: ['student', 'method', 'problem', 'report', 'learn']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'work', 'group']\n",
      "For topic  9 we have: ['method', 'model', 'system', 'image', 'electron']\n",
      "For topic 10 we have: ['circuit', 'device', 'method', 'system', 'sensor']\n"
     ]
    }
   ],
   "source": [
    "extract_topic(model_48, 5, words_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics\n",
    "- Topic 1 could be `material physics`\n",
    "- Topic 2 could be `life sciences`\n",
    "- Topic 3 could be `mathematics`\n",
    "- Topic 4 could be `evaluation`\n",
    "- Topic 5 could be `system design`\n",
    "- Topic 6 could be `learning methods`\n",
    "- Topic 7 could be `evaluation`\n",
    "- Topic 8 could be `group evaluation`\n",
    "- Topic 9 could be `imagery`\n",
    "- Topic 10 could be `electrical engineering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with LSI\n",
    "Topics are much more meaningful than with LSI but some results still leave something to be desired, especially because of the repetition of the same couple of words in most of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.9: Dirichlet hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create LDA Model with alpha variations\n",
    "@param: (rdd) RDD on which to train the model\n",
    "@param: (beta) Fixed beta (topic concentration) param\n",
    "@return: models - LDA Models trained with moving alpha param\n",
    "@return: alphas - Corresponding alpha's parameters\n",
    "\"\"\"\n",
    "def moving_alpha(rdd, beta):\n",
    "    models = []\n",
    "    alphas = []\n",
    "    for alpha in [1.01, 1.3, 1.7] + list(range(2, 21, 3)):\n",
    "        print(f'alpha: {alpha} beta: {beta}', end='\\r')\n",
    "        m = LDA.train(rdd, k=10, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)\n",
    "        models.append(m)\n",
    "        alphas.append(alpha)\n",
    "    print(f'Finished to train the {len(models)} models', end='\\r')\n",
    "    return models, alphas, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create LDA Model with beta variations\n",
    "@param: (rdd) RDD on which to train the model\n",
    "@param: (alpha) Fixed alpha (doc concentration) param\n",
    "@return: models - LDA Models trained with moving beta param\n",
    "@return: betas - Corresponding beta's parameters\n",
    "\"\"\"\n",
    "def moving_beta(rdd, alpha):\n",
    "    models = []\n",
    "    betas = []\n",
    "    for beta in [1.01, 1.3, 1.7] + list(range(2, 16, 2)):\n",
    "        print(f'alpha: {alpha} beta: {beta}', end='\\r')\n",
    "        m = LDA.train(rdd, k=10, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)\n",
    "        models.append(m)\n",
    "        betas.append(beta)\n",
    "    print(f'Finished to train the {len(models)} models', end='\\r')\n",
    "    return models, betas, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, nargs, args, narg, arg, word_ids_map):\n",
    "    for i, m in enumerate(models):\n",
    "        print(f'For model {i + 1} with parameter {nargs} {args[i]} and {narg} {arg}')\n",
    "        extract_topic(m, 5, words_ids_map)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished to train the 10 models\r"
     ]
    }
   ],
   "source": [
    "models_49a, alphas, beta = moving_alpha(rdd, 1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished to train the 10 models\r"
     ]
    }
   ],
   "source": [
    "models_49b, betas, alpha = moving_beta(rdd, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model 1 with parameter alpha 1.01 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'process', 'property', 'chemical']\n",
      "For topic  2 we have: ['student', 'cell', 'method', 'molecular', 'biology']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'equation', 'analysis']\n",
      "For topic  4 we have: ['model', 'student', 'method', 'learn', 'theory']\n",
      "For topic  5 we have: ['system', 'design', 'model', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'learn', 'innovation', 'method']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'problem', 'analysis']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'plan']\n",
      "For topic  9 we have: ['method', 'image', 'electron', 'content', 'basic']\n",
      "For topic 10 we have: ['method', 'system', 'numerical', 'circuit', 'student']\n",
      "\n",
      "\n",
      "For model 2 with parameter alpha 1.3 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'process', 'property', 'chemical']\n",
      "For topic  2 we have: ['student', 'cell', 'method', 'molecular', 'biology']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'equation', 'analysis']\n",
      "For topic  4 we have: ['model', 'student', 'method', 'learn', 'theory']\n",
      "For topic  5 we have: ['system', 'design', 'model', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'learn', 'development', 'innovation']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'problem', 'analysis']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'plan']\n",
      "For topic  9 we have: ['method', 'image', 'electron', 'content', 'basic']\n",
      "For topic 10 we have: ['method', 'system', 'numerical', 'circuit', 'device']\n",
      "\n",
      "\n",
      "For model 3 with parameter alpha 1.7 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'process', 'property', 'chemical']\n",
      "For topic  2 we have: ['cell', 'student', 'method', 'molecular', 'biology']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'equation', 'analysis']\n",
      "For topic  4 we have: ['model', 'student', 'method', 'theory', 'learn']\n",
      "For topic  5 we have: ['system', 'design', 'model', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'learn', 'development', 'innovation']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'problem', 'report']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'plan']\n",
      "For topic  9 we have: ['method', 'electron', 'image', 'content', 'model']\n",
      "For topic 10 we have: ['system', 'method', 'numerical', 'circuit', 'device']\n",
      "\n",
      "\n",
      "For model 4 with parameter alpha 2 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'process', 'property', 'chemical']\n",
      "For topic  2 we have: ['cell', 'student', 'method', 'molecular', 'biology']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'equation', 'analysis']\n",
      "For topic  4 we have: ['model', 'student', 'method', 'theory', 'learn']\n",
      "For topic  5 we have: ['system', 'design', 'model', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'learn', 'development', 'innovation']\n",
      "For topic  7 we have: ['student', 'method', 'problem', 'learn', 'report']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'plan']\n",
      "For topic  9 we have: ['method', 'electron', 'image', 'model', 'content']\n",
      "For topic 10 we have: ['system', 'method', 'numerical', 'circuit', 'device']\n",
      "\n",
      "\n",
      "For model 5 with parameter alpha 5 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'property', 'process', 'chemical']\n",
      "For topic  2 we have: ['cell', 'student', 'method', 'molecular', 'learn']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'analysis', 'system']\n",
      "For topic  4 we have: ['model', 'student', 'theory', 'method', 'learn']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'development', 'system', 'learn']\n",
      "For topic  7 we have: ['student', 'method', 'problem', 'report', 'learn']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'work']\n",
      "For topic  9 we have: ['method', 'model', 'electron', 'image', 'content']\n",
      "For topic 10 we have: ['device', 'circuit', 'system', 'method', 'numerical']\n",
      "\n",
      "\n",
      "For model 6 with parameter alpha 8 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'property', 'reaction', 'process']\n",
      "For topic  2 we have: ['cell', 'student', 'method', 'molecular', 'learn']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'data', 'analysis']\n",
      "For topic  4 we have: ['model', 'student', 'theory', 'method', 'learn']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'system', 'presentation', 'development', 'learn']\n",
      "For topic  7 we have: ['student', 'method', 'problem', 'report', 'learn']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'work']\n",
      "For topic  9 we have: ['method', 'model', 'system', 'electron', 'image']\n",
      "For topic 10 we have: ['device', 'circuit', 'system', 'method', 'sensor']\n",
      "\n",
      "\n",
      "For model 7 with parameter alpha 11 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'property', 'reaction', 'chemical']\n",
      "For topic  2 we have: ['student', 'cell', 'method', 'learn', 'molecular']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'data', 'processing']\n",
      "For topic  4 we have: ['model', 'student', 'method', 'learn', 'theory']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'system', 'learn', 'method', 'paper']\n",
      "For topic  7 we have: ['student', 'method', 'report', 'problem', 'learn']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'method', 'data']\n",
      "For topic  9 we have: ['model', 'method', 'system', 'electron', 'design']\n",
      "For topic 10 we have: ['device', 'circuit', 'system', 'method', 'sensor']\n",
      "\n",
      "\n",
      "For model 8 with parameter alpha 14 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'laser', 'student', 'property']\n",
      "For topic  2 we have: ['student', 'method', 'cell', 'learn', 'molecular']\n",
      "For topic  3 we have: ['learn', 'method', 'data', 'model', 'student']\n",
      "For topic  4 we have: ['student', 'model', 'method', 'learn', 'stochastic']\n",
      "For topic  5 we have: ['system', 'model', 'method', 'design', 'learn']\n",
      "For topic  6 we have: ['student', 'system', 'method', 'learn', 'content']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'report', 'problem']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'method', 'learn']\n",
      "For topic  9 we have: ['system', 'model', 'method', 'design', 'learn']\n",
      "For topic 10 we have: ['device', 'circuit', 'method', 'system', 'sensor']\n",
      "\n",
      "\n",
      "For model 9 with parameter alpha 17 and beta 1.01\n",
      "For topic  1 we have: ['material', 'energy', 'student', 'laser', 'learn']\n",
      "For topic  2 we have: ['student', 'method', 'learn', 'cell', 'protein']\n",
      "For topic  3 we have: ['learn', 'method', 'student', 'data', 'model']\n",
      "For topic  4 we have: ['student', 'model', 'method', 'learn', 'stochastic']\n",
      "For topic  5 we have: ['system', 'method', 'model', 'student', 'learn']\n",
      "For topic  6 we have: ['student', 'system', 'method', 'learn', 'content']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'model', 'problem']\n",
      "For topic  8 we have: ['student', 'project', 'design', 'method', 'learn']\n",
      "For topic  9 we have: ['system', 'model', 'method', 'student', 'design']\n",
      "For topic 10 we have: ['device', 'method', 'circuit', 'system', 'material']\n",
      "\n",
      "\n",
      "For model 10 with parameter alpha 20 and beta 1.01\n",
      "For topic  1 we have: ['material', 'student', 'learn', 'energy', 'design']\n",
      "For topic  2 we have: ['student', 'method', 'learn', 'cell', 'protein']\n",
      "For topic  3 we have: ['learn', 'student', 'method', 'data', 'model']\n",
      "For topic  4 we have: ['student', 'method', 'model', 'learn', 'stochastic']\n",
      "For topic  5 we have: ['system', 'method', 'student', 'model', 'learn']\n",
      "For topic  6 we have: ['student', 'system', 'method', 'learn', 'content']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'model', 'risk']\n",
      "For topic  8 we have: ['student', 'project', 'design', 'method', 'learn']\n",
      "For topic  9 we have: ['system', 'method', 'model', 'student', 'design']\n",
      "For topic 10 we have: ['method', 'device', 'student', 'system', 'material']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(models_49a, 'alpha', alphas, 'beta', beta, words_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that high alpha means more homogeneous distribution of words per topic, which could be explained by the homogeneous distribution of topics per document it causes according to the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model 1 with parameter beta 1.01 and alpha 6\n",
      "For topic  1 we have: ['material', 'energy', 'property', 'process', 'chemical']\n",
      "For topic  2 we have: ['cell', 'student', 'method', 'molecular', 'learn']\n",
      "For topic  3 we have: ['learn', 'model', 'method', 'analysis', 'system']\n",
      "For topic  4 we have: ['model', 'student', 'theory', 'method', 'learn']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'presentation', 'system', 'development', 'paper']\n",
      "For topic  7 we have: ['student', 'method', 'problem', 'report', 'learn']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'data', 'work']\n",
      "For topic  9 we have: ['method', 'model', 'electron', 'system', 'image']\n",
      "For topic 10 we have: ['device', 'circuit', 'system', 'method', 'sensor']\n",
      "\n",
      "\n",
      "For model 2 with parameter beta 1.3 and alpha 6\n",
      "For topic  1 we have: ['material', 'energy', 'property', 'chemical', 'process']\n",
      "For topic  2 we have: ['student', 'cell', 'molecular', 'method', 'biology']\n",
      "For topic  3 we have: ['learn', 'method', 'processing', 'system', 'model']\n",
      "For topic  4 we have: ['model', 'student', 'learn', 'method', 'exam']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'learn', 'method', 'paper', 'presentation']\n",
      "For topic  7 we have: ['student', 'method', 'data', 'report', 'problem']\n",
      "For topic  8 we have: ['project', 'student', 'design', 'work', 'method']\n",
      "For topic  9 we have: ['method', 'model', 'image', 'system', 'flow']\n",
      "For topic 10 we have: ['circuit', 'method', 'device', 'system', 'numerical']\n",
      "\n",
      "\n",
      "For model 3 with parameter beta 1.7 and alpha 6\n",
      "For topic  1 we have: ['material', 'energy', 'property', 'chemical', 'process']\n",
      "For topic  2 we have: ['student', 'cell', 'molecular', 'method', 'learn']\n",
      "For topic  3 we have: ['system', 'learn', 'method', 'student', 'processing']\n",
      "For topic  4 we have: ['student', 'model', 'learn', 'method', 'exam']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'presentation', 'content']\n",
      "For topic  7 we have: ['student', 'method', 'data', 'learn', 'project']\n",
      "For topic  8 we have: ['student', 'project', 'design', 'method', 'learn']\n",
      "For topic  9 we have: ['method', 'model', 'image', 'system', 'learn']\n",
      "For topic 10 we have: ['method', 'circuit', 'system', 'microscopy', 'model']\n",
      "\n",
      "\n",
      "For model 4 with parameter beta 2 and alpha 6\n",
      "For topic  1 we have: ['material', 'energy', 'chemical', 'process', 'property']\n",
      "For topic  2 we have: ['student', 'cell', 'method', 'molecular', 'learn']\n",
      "For topic  3 we have: ['system', 'learn', 'method', 'student', 'design']\n",
      "For topic  4 we have: ['student', 'learn', 'method', 'model', 'content']\n",
      "For topic  5 we have: ['system', 'model', 'design', 'method', 'learn']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'content']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'project', 'data']\n",
      "For topic  8 we have: ['student', 'project', 'method', 'design', 'learn']\n",
      "For topic  9 we have: ['method', 'model', 'system', 'learn', 'student']\n",
      "For topic 10 we have: ['method', 'system', 'circuit', 'model', 'microscopy']\n",
      "\n",
      "\n",
      "For model 5 with parameter beta 4 and alpha 6\n",
      "For topic  1 we have: ['student', 'learn', 'method', 'material', 'energy']\n",
      "For topic  2 we have: ['student', 'method', 'learn', 'content', 'cell']\n",
      "For topic  3 we have: ['student', 'method', 'learn', 'system', 'design']\n",
      "For topic  4 we have: ['student', 'learn', 'method', 'content', 'model']\n",
      "For topic  5 we have: ['method', 'model', 'student', 'system', 'learn']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic  8 we have: ['student', 'method', 'learn', 'content', 'design']\n",
      "For topic  9 we have: ['method', 'model', 'student', 'learn', 'system']\n",
      "For topic 10 we have: ['method', 'student', 'learn', 'model', 'system']\n",
      "\n",
      "\n",
      "For model 6 with parameter beta 6 and alpha 6\n",
      "For topic  1 we have: ['student', 'learn', 'method', 'system', 'content']\n",
      "For topic  2 we have: ['student', 'content', 'method', 'learn', 'cell']\n",
      "For topic  3 we have: ['student', 'method', 'learn', 'content', 'system']\n",
      "For topic  4 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  5 we have: ['method', 'student', 'learn', 'model', 'content']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic  8 we have: ['student', 'method', 'content', 'learn', 'model']\n",
      "For topic  9 we have: ['method', 'student', 'model', 'learn', 'system']\n",
      "For topic 10 we have: ['method', 'student', 'learn', 'content', 'model']\n",
      "\n",
      "\n",
      "For model 7 with parameter beta 8 and alpha 6\n",
      "For topic  1 we have: ['student', 'learn', 'method', 'system', 'content']\n",
      "For topic  2 we have: ['content', 'student', 'method', 'learn', 'basic']\n",
      "For topic  3 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  4 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  5 we have: ['student', 'method', 'content', 'learn', 'model']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic  8 we have: ['student', 'method', 'content', 'learn', 'model']\n",
      "For topic  9 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic 10 we have: ['method', 'student', 'learn', 'content', 'model']\n",
      "\n",
      "\n",
      "For model 8 with parameter beta 10 and alpha 6\n",
      "For topic  1 we have: ['student', 'method', 'learn', 'system', 'content']\n",
      "For topic  2 we have: ['student', 'method', 'content', 'learn', 'model']\n",
      "For topic  3 we have: ['student', 'method', 'learn', 'model', 'content']\n",
      "For topic  4 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  5 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  8 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  9 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic 10 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "\n",
      "\n",
      "For model 9 with parameter beta 12 and alpha 6\n",
      "For topic  1 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  2 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  3 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic  4 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  5 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  8 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  9 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic 10 we have: ['student', 'method', 'learn', 'model', 'content']\n",
      "\n",
      "\n",
      "For model 10 with parameter beta 14 and alpha 6\n",
      "For topic  1 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  2 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  3 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic  4 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  5 we have: ['student', 'method', 'learn', 'model', 'content']\n",
      "For topic  6 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  7 we have: ['student', 'method', 'learn', 'system', 'model']\n",
      "For topic  8 we have: ['student', 'method', 'learn', 'content', 'model']\n",
      "For topic  9 we have: ['student', 'method', 'learn', 'model', 'system']\n",
      "For topic 10 we have: ['student', 'method', 'learn', 'model', 'content']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(models_49b, 'beta', betas, 'alpha', alpha, words_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that high beta means an homogeneous distribution of words per topic, and pretty quickly all topics are almost equivalent, which can be explained with the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.10: EPFL's taught subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exercise `4.9` we found out that parameters that give the most interpretable results are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "alpha = 1.1\n",
    "beta = 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k = 15 because 13 is the number of sections at EPFL, therefore we expect around this number of topics for course-related information and a few more relating to administration or other.\n",
    "- alpha = 1.1 because we saw we needed a low alpha\n",
    "- beta = 1.01 because we saw we needed a low beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_410 = LDA.train(rdd, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic  1 we have: ['reaction', 'material', 'method', 'spectroscopy', 'chemistry']\n",
      "For topic  2 we have: ['student', 'cell', 'biology', 'note', 'microscopy']\n",
      "For topic  3 we have: ['method', 'numerical', 'equation', 'model', 'problem']\n",
      "For topic  4 we have: ['student', 'project', 'report', 'learn', 'data']\n",
      "For topic  5 we have: ['linear', 'model', 'method', 'image', 'probability']\n",
      "For topic  6 we have: ['material', 'design', 'circuit', 'device', 'application']\n",
      "For topic  7 we have: ['design', 'method', 'market', 'student', 'paper']\n",
      "For topic  8 we have: ['student', 'work', 'project', 'design', 'learn']\n",
      "For topic  9 we have: ['chemical', 'student', 'learn', 'process', 'method']\n",
      "For topic 10 we have: ['energy', 'student', 'system', 'process', 'conversion']\n",
      "For topic 11 we have: ['optical', 'optic', 'control', 'light', 'laser']\n",
      "For topic 12 we have: ['model', 'method', 'analysis', 'signal', 'dynamic']\n",
      "For topic 13 we have: ['student', 'data', 'learn', 'project', 'method']\n",
      "For topic 14 we have: ['student', 'method', 'learn', 'energy', 'policy']\n",
      "For topic 15 we have: ['system', 'model', 'design', 'learn', 'student']\n"
     ]
    }
   ],
   "source": [
    "extract_topic(model_410, 5, words_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics\n",
    "\n",
    "- Topic 1 `chemistry`\n",
    "- Topic 2 `life sciences`\n",
    "- Topic 3 `mathematics`\n",
    "- Topic 4 `evaluation`\n",
    "- Topic 5 `machine learning`\n",
    "- Topic 6 `micro-technique`\n",
    "- Topic 7 `MTE research`\n",
    "- Topic 8 `student work`\n",
    "- Topic 9 `chemitry experiments`\n",
    "- Topic 10 `physics experiment`\n",
    "- Topic 11 `optics`\n",
    "- Topic 12 `signal processing`\n",
    "- Topic 13 `learning methods`\n",
    "- Topic 14 `energy policy`\n",
    "- Topic 15 `modelisation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_rdd = sc.textFile('/ix/wikipedia-for-schools.txt').map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create super set bag of words\n",
    "@param: (rdd) RDD dataset of courses\n",
    "@return: RDD of all existing terms\n",
    "\"\"\"\n",
    "def bow_wiki(rdd):\n",
    "    return set(rdd.flatMap(lambda c: c[\"tokens\"]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_superbow = bow_wiki(wikipedia_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(wiki_superbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids_map = dict(zip(wiki_superbow, range(len(wiki_superbow))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the sparse vector of term counters for the given page\n",
    "@param: (page) Fixed alpha (doc concentration) param\n",
    "@return: sparse term counter vector\n",
    "\"\"\"\n",
    "def create_term_count_vector(page):\n",
    "    terms_count = Counter(page['tokens'])\n",
    "    indices = []\n",
    "    values = []\n",
    "    for w, c in terms_count.items():\n",
    "        indices.append(tokens_ids_map[w])\n",
    "        values.append(c)\n",
    "         \n",
    "    args_sorted = np.argsort(indices)\n",
    "    values = np.array(values)[args_sorted]\n",
    "    indices = np.array(indices)[args_sorted]\n",
    "    return Vectors.sparse(dim, indices, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_rdd = wikipedia_rdd.zipWithIndex().map(lambda t: [t[1], create_term_count_vector(t[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4.11: Wikipedia structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our interpretations we think that optimal parameters giving most interpretable results are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "alpha = 1.05\n",
    "beta = 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose :\n",
    "- k = 20 because as seen here (https://schools-wikipedia.org/wp/index/subject.html) there are 16 subjects covered in the dataset, and we expect a few subtilities will be uncovered\n",
    "- alpha = 1.05 because we expect a really un-uniform topic distribution per article\n",
    "- beta = 1.01 because we wxpect a reaaly un-uniform word distribution per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_411 = LDA.train(wiki_rdd, k=k, docConcentration=float(alpha), topicConcentration=float(beta), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For topic  1 we have: ['time', 'king', 'film', 'series', 'death', 'album', 'war', 'years', 'early', 'work']\n",
      "For topic  2 we have: ['games', 'game', 'united', 'music', 'states', 'company', 'world', 'windows', 'country', 'computer']\n",
      "For topic  3 we have: ['species', 'human', 'cells', 'blood', 'disease', 'common', 'cell', 'called', 'study', 'found']\n",
      "For topic  4 we have: ['–', 'river', 'open', 'world', 'years', 'won', 'time', 'final', 'grand', 'year']\n",
      "For topic  5 we have: ['water', 'sea', 'species', 'north', 'island', 'hurricane', 'lake', 'south', 'ocean', 'tropical']\n",
      "For topic  6 we have: ['city', 'government', 'population', 'area', 'national', 'largest', 'capital', 'centre', 'economic', 'world']\n",
      "For topic  7 we have: ['earth', 'sun', 'solar', 'years', 'stars', 'planet', 'star', 'planets', 'mass', 'church']\n",
      "For topic  8 we have: ['energy', '·', 'law', 'theory', 'light', 'system', 'time', 'mass', 'universe', 'force']\n",
      "For topic  9 we have: ['american', 'war', '–', 'british', 'french', 'january', 'march', 'july', 'united', 'june']\n",
      "For topic 10 we have: ['acid', 'water', 'states', 'carbon', 'production', 'chemical', 'state', 'gas', 'form', 'high']\n",
      "For topic 11 we have: ['language', 'chinese', 'languages', 'century', 'china', 'english', 'called', 'ancient', 'latin', 'dynasty']\n",
      "For topic 12 we have: ['god', 'church', 'century', 'political', 'empire', 'soviet', 'war', 'time', 'russian', 'roman']\n",
      "For topic 13 we have: ['south', 'species', 'africa', 'area', 'population', 'north', 'world', 'people', 'large', 'areas']\n",
      "For topic 14 we have: ['film', 'art', 'work', 'life', 'works', 'time', 'show', 'world', 'people', 'early']\n",
      "For topic 15 we have: ['oil', 'time', 'number', 'work', 'called', 'order', 'life', 'century', 'production', 'group']\n",
      "For topic 16 we have: ['team', 'game', 'player', 'players', 'england', 'play', 'played', 'football', 'ball', 'world']\n",
      "For topic 17 we have: ['–', 'time', 'war', 'world', 'years', 'series', 'united', 'school', 'president', 'john']\n",
      "For topic 18 we have: ['=', 'number', 'theory', 'function', 'numbers', 'set', 'called', 'form', 'point', 'mathematics']\n",
      "For topic 19 we have: ['american', 'soviet', 'october', 'time', 'space', 'december', '–', 'aircraft', 'war', '·']\n",
      "For topic 20 we have: ['music', 'city', 'bridge', 'century', 'instruments', 'time', 'years', 'musical', 'river', 'made']\n"
     ]
    }
   ],
   "source": [
    "extract_topic(model_411, 10, tokens_ids_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label topics\n",
    "\n",
    "- Topic 1 `films and series`\n",
    "- Topic 2 `computer games`\n",
    "- Topic 3 `life sciences`\n",
    "- Topic 4 `geology`\n",
    "- Topic 5 `places relating to water`\n",
    "- Topic 6 `geo-political`\n",
    "- Topic 7 `solar system`\n",
    "- Topic 8 `physics`\n",
    "- Topic 9 `american independance war`\n",
    "- Topic 10 `chemistry`\n",
    "- Topic 11 `languages in history`\n",
    "- Topic 12 `religion and politics`\n",
    "- Topic 13 `unspecified`\n",
    "- Topic 14 `arty films`\n",
    "- Topic 15 `industrial revolution`\n",
    "- Topic 16 `football`\n",
    "- Topic 17 `wars`\n",
    "- Topic 18 `mathematics`\n",
    "- Topic 19 `space race`\n",
    "- Topic 20 `music`\n",
    "\n",
    "We are quite happy with the results as most of the clusters are quite recognizable, but results could surely be improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
